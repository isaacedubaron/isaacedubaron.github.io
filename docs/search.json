[
  {
    "objectID": "posts/Problem Set 1/index.html",
    "href": "posts/Problem Set 1/index.html",
    "title": "Problem Set 1 | ",
    "section": "",
    "text": "The purpose of this document is to simulataneously analyze data on US crime rates and become more familiar with the syntax and abilities of R-markdown to combine code and analysis in a progressional document. Blockquotes look better in HTML typically, but you can see their general effect in any document. The text is highlighted differently in RStudio so you know its part of the block quote. Also, the margins of the text in the final document are narrower to separate the block quote from normal text."
  },
  {
    "objectID": "posts/Problem Set 1/index.html#summary-of-features",
    "href": "posts/Problem Set 1/index.html#summary-of-features",
    "title": "Problem Set 1 | ",
    "section": "Summary of Features",
    "text": "Summary of Features\n\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\n\nMin. : 0.800\nMin. : 45.0\nMin. :32.00\nMin. : 7.30\n\n\n\n1st Qu.: 4.075\n1st Qu.:109.0\n1st Qu.:54.50\n1st Qu.:15.07\n\n\n\nMedian : 7.250\nMedian :159.0\nMedian :66.00\nMedian :20.10\n\n\n\nMean : 7.788\nMean :170.8\nMean :65.54\nMean :21.23\n\n\n\n3rd Qu.:11.250\n3rd Qu.:249.0\n3rd Qu.:77.75\n3rd Qu.:26.18\n\n\n\nMax. :17.400\nMax. :337.0\nMax. :91.00\nMax. :46.00\n\n\n\n\n\n\nAcross all 50 states the mean of the Murder variable is 7.79 arrests for murder per 100,000 people. While the mean of Assault is 170.8 arrests per 100,000 people. The mean of Rape is 21.23 arrests per 100,000 people. While the mean of UrbanPop is 65.54 per 100,000 people.\n\n# Make sure that this code block shows up in the final document\n# and that the resulting plot does also.\nlibrary(ggplot2)\nlibrary(tidyr)\nscaled_data = as.data.frame(sapply(USArrests, scale))\nggplot(gather(scaled_data, cols, value), aes(x = value)) + \n       geom_histogram(aes(y=..density..), bins = 10) + \n       geom_density(alpha=.2, fill=\"#FF6666\") +\n       facet_grid(.~cols) +\n       ggtitle(\"Feature Histograms for the Scaled US Arrests Data\")\n\n\n\n\n\nMurder is right-skewed. Assault is right-skewed. UrbanPop is approximately symmetric. Rape is right-skewed."
  },
  {
    "objectID": "posts/Problem Set 1/index.html#relationships-between-features",
    "href": "posts/Problem Set 1/index.html#relationships-between-features",
    "title": "Problem Set 1 | ",
    "section": "Relationships Between Features",
    "text": "Relationships Between Features\n\n\n\n\n\nFacet Grid of Scatter Plots\n\n\n\n\n\nThere appears to be a positively correlated relationship between Murder and Assault. The appears to also be a positively correlated relationship between UrbanPop and the other three arrest variables Murder, Assault, and Rape.\n\n\n\n\n\nVariable\nMean\n\n\n\n\nMurder\n7.788\n\n\nAssault\n170.76\n\n\nUrbanPop\n65.54\n\n\nRape\n21.232"
  },
  {
    "objectID": "posts/Problem Set 1/index.html#what-are-the-7-basic-steps-of-machine-learning",
    "href": "posts/Problem Set 1/index.html#what-are-the-7-basic-steps-of-machine-learning",
    "title": "Problem Set 1 | ",
    "section": "What are the 7 basic steps of machine learning?",
    "text": "What are the 7 basic steps of machine learning?\n\nData Collection\nData Reprocessing\nSplitting the Data\nModel Selection\nModel Training\nModel Evaluation\nModel Deployment"
  },
  {
    "objectID": "posts/Problem Set 1/index.html#in-your-own-words-please-explain-the-bias-variance-tradeoff-in-supervised-machine-learning-and-make-sure-to-include-proper-terminology.",
    "href": "posts/Problem Set 1/index.html#in-your-own-words-please-explain-the-bias-variance-tradeoff-in-supervised-machine-learning-and-make-sure-to-include-proper-terminology.",
    "title": "Problem Set 1 | ",
    "section": "In your own words, please explain the bias-variance tradeoff in supervised machine learning and make sure to include proper terminology.",
    "text": "In your own words, please explain the bias-variance tradeoff in supervised machine learning and make sure to include proper terminology.\nThe bias-variance trade-off: it refers to the balance that you need to strike between two sources of error when building a model: Bias: Bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to under fitting, where the model is too simple to capture the underlying patterns in the data. It results in poor performance on both the training and validation sets. Variance: Variance is the error introduced by the models sensitivity to small fluctuations or noise in the training data. High variance can lead to over fitting, where the model becomes too complex and fits the training data too closely. It performs well on the training data but poorly on the validation or test data. The goal in machine learning is to find a model that achieves a balance between bias and variance. This is because reducing bias often increases variance, and vice versa. The challenge is to select the right complexity of the model and fine-tune its parameters to minimize both bias and variance, resulting in a model that generalizes well to unseen data."
  },
  {
    "objectID": "posts/Problem Set 1/index.html#explain-in-your-own-words-why-cross-validation-is-important-and-useful.",
    "href": "posts/Problem Set 1/index.html#explain-in-your-own-words-why-cross-validation-is-important-and-useful.",
    "title": "Problem Set 1 | ",
    "section": "Explain, in your own words, why cross-validation is important and useful.",
    "text": "Explain, in your own words, why cross-validation is important and useful.\nCross-validation is essential machine because: Cross-validation provides a more robust estimate of a models performance compared to a single train-test split. It helps in assessing how well a model generalizes to different subsets of the data. Maximizing Data Utilization: By rotating through different subsets of the data as training and validation sets, cross-validation ensures that all available data is used for both training and evaluation, which is particularly important when the dataset is limited."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named “index.qmd” insides of folder with the name of the post. For example, if I wanted my post to be titled “Demo Post 1” then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart’s content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the “Demo Post 1” folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set 3\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nIsaac Baron\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set 2 | \n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nIsaac Baron [ibaron01@hamline.edu - Student ]\n\n\n\n\n\n\n  \n\n\n\n\nProblem Set 1 | \n\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nIsaac Baron [ibaron01@hamline.edu - Student ]\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Baron",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the “index” page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Problem Set 2/index.html",
    "href": "posts/Problem Set 2/index.html",
    "title": "Problem Set 2 | ",
    "section": "",
    "text": "Before getting started, we’ll need to make sure the necessary packages installed and libraries loaded. Look at the list below and install if necessary before loading them.\n# Clear everything\nrm(list = ls())\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(GGally)\nlibrary(dummy)\nlibrary(corrplot)"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-1",
    "href": "posts/Problem Set 2/index.html#step-1",
    "title": "Problem Set 2 | ",
    "section": "Step 1",
    "text": "Step 1\nIn this step we are seeking some understanding of the data we have obtained. Remember this is different from understanding the data we NEED to obtain to best answer a business question. We need to understand both and the differences between the two. But, because we have this bikes_ps.csv data set, we will take a dive into that. We first just get an idea for the dimensions and contents.\n\n# Read in the data set and use glimpse to get an idea.\nbikes = read_csv(\"bikes_ps.csv\")\nglimpse(bikes)\n\nRows: 731\nColumns: 10\n$ date        &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-0…\n$ season      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ holiday     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ weekday     &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4…\n$ weather     &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2…\n$ temperature &lt;dbl&gt; 46.71653, 48.35024, 34.21239, 34.52000, 36.80056, 34.88784…\n$ realfeel    &lt;dbl&gt; 46.39865, 45.22419, 25.70131, 28.40009, 30.43728, NA, 28.0…\n$ humidity    &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261…\n$ windspeed   &lt;dbl&gt; 6.679665, 10.347140, 10.337565, 6.673420, 7.780994, 3.7287…\n$ rentals     &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 12…\n\n\nThe outcome of glimpse() tells us immediately that our data set contains\n\n731 observations with 10 columns (or features).\nData types are initially all numeric &lt;dbl&gt; except for a date which is a &lt;date&gt; data type. this is code\n\nNow these are the default choices made by the structure of the data set along with the processing intelligence of the read_csv() import function. But our human understanding of the data and its use in solving a business problem are crucial to understanding what the datatype should be and whether changes will need to be made."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-2",
    "href": "posts/Problem Set 2/index.html#step-2",
    "title": "Problem Set 2 | ",
    "section": "Step 2",
    "text": "Step 2\nLooking at the data we see that the numeric data types are not truly appropriate for some of the variables. Lets start with some obvious ones like season and holiday. First we need to remember that all variables/features are encoded information. And we need to discern what the original information to be encoded was and how the encoding scheme we see relates to it. For season we can be confident it was meant to indicate the general season in which a rental took place, such as winter, spring, summer, and fall. Instead of using text, these seasons were encoded, or represented by, a number. Now any numbers could be chosen, but typically the numbers might start at the beginning of a year and progress from there, in other words maybe winter = 1, spring = 2, etc. However, we cannot be truly certain without checking. First the best idea is to look at a codebook if one is provided. A codebook is a description of encoding schemes given by the person or persons who actually did the encoding. An alternative method in this case would be to look at the data column along with the season to see whether 1 corresponds to winter months etc. For brevity, we won’t do this for all features, but we’ll take a look at what it means to sleuth out these problems.\n\n# Create a new feature representing the month of year\n# i.e., jan = 1, feb = 2, ..., dec = 12.\n# Then we'll create a table showing season by month\nbikes %&gt;%\n  mutate(month = month(date)) %&gt;%\n  group_by(month) %&gt;%\n  select(season, month) %&gt;%\n  table()\n\n      month\nseason  1  2  3  4  5  6  7  8  9 10 11 12\n     1 62 57 40  0  0  0  0  0  0  0  0 22\n     2  0  0 22 60 62 40  0  0  0  0  0  0\n     3  0  0  0  0  0 20 62 62 44  0  0  0\n     4  0  0  0  0  0  0  0  0 16 62 60 40\n\n\nFrom the above table it becomes clear that the season variable not easily dividable into months. For example season 1 does correspond to wintery months such as December, January, Februrary, and March. But March also has some season 2. Similarly, December has a lot more observations in season 4 (maybe Fall?) than season 1 - Winter. This may suggest that the variable indicates the first official day of winter on December 21 and the first official day of spring on March 21, etc.\nHowever it is encoded, the season feature is not truly numeric. Instead a number on a football jersey, the value is nominal and meant to be an identifier - identifying to which season a day belongs. This is called nominal or categorical data. In R, this is most commonly coded as the factor datatype.\nOther features also use numbers this way and should be represented as factors instead: holiday, weekday, and weather. We can now convert these to factors, and even specify new labels if we’d like.\n\nbikes = bikes %&gt;%\n  mutate_at(vars(season, holiday, weekday, weather), factor) %&gt;%\n  mutate(season = fct_recode(season, \"Winter\"=\"1\", \n                                     \"Spring\"=\"2\",\n                                     \"Summer\"=\"3\",\n                                     \"Fall\"=\"4\"))\n\n\nOther Factor Feature Explanations\n\nholiday: This is a binary indicator. A “1” indicates the data is considered a holiday and a 0 that it isn’t. This is categorical and so it needed to be converted to two groups.\nweekday: Here each number represents a day of the week like Sunday, Monday, etc. This means the numbers don’t act as numbers - but instead indicate the day of the week a rental occurs. So we convert it to the categorical factor data type. We could easily change the labels to reflect the day of the week.\nweather: This appears to take on values 1, 2, and 3. But what does it mean? Without a codebook this one is a problem. We cannot be sure whether this is in fact categorical or numerical, and we wouldn’t know that the categories are. Likely, it refers to weather severity or precipitation. For example, perhaps 1 is clear skies no precipitation, 2 is cloudy/rainy, and 3 is stormy. But we would need to reach out to the data creator to be sure."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-3",
    "href": "posts/Problem Set 2/index.html#step-3",
    "title": "Problem Set 2 | ",
    "section": "Step 3",
    "text": "Step 3\nNow that we’ve got everything properly recognized as numeric or factor, we can use summary() to look at some basic statistics and also scout out missing values. Do make things easier to read, we’ll divide summaries by numeric and factor data types.\n\nbikes %&gt;%\n  select(-date) %&gt;%\n  keep(is.numeric) %&gt;%\n  summary()\n\n  temperature       realfeel         humidity        windspeed      \n Min.   :22.60   Min.   : 12.59   Min.   :0.0000   Min.   : 0.9322  \n 1st Qu.:46.12   1st Qu.: 43.38   1st Qu.:0.5200   1st Qu.: 5.6182  \n Median :59.76   Median : 61.25   Median :0.6267   Median : 7.5343  \n Mean   :59.51   Mean   : 59.60   Mean   :0.6279   Mean   : 7.9303  \n 3rd Qu.:73.05   3rd Qu.: 75.43   3rd Qu.:0.7302   3rd Qu.: 9.7092  \n Max.   :90.50   Max.   :103.10   Max.   :0.9725   Max.   :21.1266  \n                 NA's   :27                                         \n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n               \n\n\n\nbikes %&gt;%\n  select(-date) %&gt;%\n  keep(is.factor) %&gt;%\n  summary()\n\n    season    holiday weekday weather\n Winter:181   0:710   0:105   1:463  \n Spring:184   1: 21   1:105   2:247  \n Summer:188           2:104   3: 21  \n Fall  :178           3:104          \n                      4:104          \n                      5:104          \n                      6:105          \n\n\nWe see that we have no missing values for factor variables, and are only missing values for the realfeel variable in the set of numeric variables. We are missing 27 values. We could throw these out, but one problem with that is what if they are not missing by random? In other words, what if there are certain days, say when rentals are really high or low that causes this number not to be recorded? Also, although 27 observations are missing realfeel, they are not missing other values. By discarding them, we also throw out all the other information those observations contain. An alternative is to impute the missing values. This means we fill in numbers in the blank spots. But what numbers? We’re essentially making up data by trying to guess what was supposed to be recorded there. If we’re going to do this, we should first try to do no harm. Essentially, we should hope that the statistical properties of the data are not altered or biased by our choice of value. There are number of ways to do this, but for this assignment you’re asked to do the median value imputation. For illustration purposes I’m going to create a copy to compare (you don’t need to do this).\n\nbikes = bikes %&gt;%\n  mutate(realfeel_orig = realfeel)\n\nNow, lets impute the missing values and compare.\n\nbikes = bikes %&gt;%\n  mutate(realfeel = ifelse(is.na(realfeel),\n                           median(realfeel, na.rm = TRUE),\n                           realfeel))\n\nThe above code uses ifelse logic to replace values. It asks a question (checks a condition) and then does different actions based on the answer.\nIs realfeel missing? (is.na(realfeel)):\n\nYES (TRUE): replace with median(reelfeel, na.rm = TRUE).\nNO (FALSE): replace with realfeel (which leaves it unchanged, since we’re just replacing it with itself).\n\nNow we can compare the resulting distributions.\n\nbikes %&gt;%\n  select(realfeel, realfeel_orig) %&gt;%\n  summary()\n\n    realfeel      realfeel_orig   \n Min.   : 12.59   Min.   : 12.59  \n 1st Qu.: 43.80   1st Qu.: 43.38  \n Median : 61.25   Median : 61.25  \n Mean   : 59.66   Mean   : 59.60  \n 3rd Qu.: 74.98   3rd Qu.: 75.43  \n Max.   :103.10   Max.   :103.10  \n                  NA's   :27      \n\n\nLooking at the above distributions, we see that realfeel doesn’t have any missing values and is the same median and basically the same mean. Extreme points are not changed, although the 1st and 3rd quartiles changed a smidgen.\n\n# Remove the copy of original realfeel\nbikes = bikes %&gt;% select(-realfeel_orig)"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-4",
    "href": "posts/Problem Set 2/index.html#step-4",
    "title": "Problem Set 2 | ",
    "section": "Step 4",
    "text": "Step 4\nNow we need to gain some understanding of what we’re trying to predict, rentals. This involves understanding what the variable is and its distribution. Rentals appears to encode the total numbers of bike rentals that occurred on a given date. This is count data. We can use both descriptive statistics as well as a histogram to get a picture. Additionally, we can look at a picture of rentals over time to see if there is some trending.\n\nbikes %&gt;% select(rentals) %&gt;% summary()\n\n    rentals    \n Min.   :  22  \n 1st Qu.:3152  \n Median :4548  \n Mean   :4504  \n 3rd Qu.:5956  \n Max.   :8714  \n\n\nThe lowest recorded number is 22 rentals, and the max a whopping 8,714 rentals! Across the data the mean is roughly 4500 rentals and the median is only a little higher suggesting that there shouldn’t be an extreme skew and it’s fairly symmetric.\n\nbikes %&gt;%\n  ggplot(aes(x=rentals)) + \n  geom_histogram(aes(y=after_stat(density)),\n                 fill = \"aquamarine\",\n                 color = \"aquamarine3\",\n                 alpha = 0.7) +\n  geom_density(color = \"black\") +\n  labs(title = \"Distribution of Daily Bike Rentals\",\n       x = \"Rentals (count)\") +\n  theme_clean()\n\n\n\n\nFortunately, we don’t seem to have a huge number of outliers and the distribution is not highly skewed. This means that we might not need to make a log-transformation of this feature to make it more normal. However, one thing to note is that it is a tri-model looking distribution. There are peaks in the data which suggest that there might be three different over-lapping normal distributions. A low, middle, and high one."
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-5",
    "href": "posts/Problem Set 2/index.html#step-5",
    "title": "Problem Set 2 | ",
    "section": "Step 5",
    "text": "Step 5\nMany of the supervised learning algorithms can be helped or hurt by the relationships between features that will be used as predictors. We need to understand the distributions of each variable, looking for skew, outliers, and any other weirdness. This could involve histograms or boxplots of the variables. We can use scatter plots to look at relationships between predictors. For easier comparison we can also use correlation matrices to show statistically linear relationships.\n\nbikes %&gt;%\n  keep(is.numeric) %&gt;%\n  ggpairs()\n\n\n\n\nFirst off we can see that temperature and realfeel have an almost perfectly linear relationship. The correlation is 0.96! This is a suspiciously strong relationship. In fact, this usually means that one variable is a function of the other. Indeed, realfeel is a relationship between temperature and humidity and wind that is mean to incorporate what temperature it feels like to a human. In such a case, we will want to leave out a variable. Either realfeel or the other features that go into it.\nThe distribution plots do not look particularly alarming. And the scatterplots don’t show any other overwhelmingly strong relationships. What we can see, is that there is a positive and nonlinear relationship between temperature and rentals. Warmer temps are associated with more rentals (not surprising). But eventually, warmer temperatures result in weather that is too hot for comfort - leading to decreased rentals.\nWe can also check these correlations with corrplot.\n\nbikes %&gt;%\n  keep(is.numeric) %&gt;%\n  cor() %&gt;%\n  corrplot()\n\n\n\n\nSometimes we need to convert features to achieve different objectives.\n\nWe might transform a feature to make it easier for our learning algorithm to use, or\nwe might transform a feature to put it on the same or similar scale with the the other features.\n\nWe’re going to Z-score normalize the temperature feature. Our reason is mostly arbitrary, but one benefit is that after the transformation the mean will be zero. Positive numbers will represent above average temperatures and negative below average ones.\n\nbikes = bikes %&gt;%\n  mutate(temperature = (temperature - mean(temperature))/sd(temperature))\n\nbikes %&gt;%\n  select(temperature) %&gt;%\n  summary()\n\n  temperature      \n Min.   :-2.38324  \n 1st Qu.:-0.86479  \n Median : 0.01611  \n Mean   : 0.00000  \n 3rd Qu.: 0.87425  \n Max.   : 2.00098  \n\n\nWe can min-max normalize the wind variable. This will take all values of the feature and cram it into the interval \\([0, 1]\\). It essentially puts a feature into a percent range.\n\nbikes = bikes %&gt;%\n  mutate(windspeed = (windspeed - min(windspeed))/(max(windspeed)-min(windspeed)))\n\nA very important step, and a very common one required by many learning algorithms, is converting all categorical variables into dummy variables. This can be done many different ways in R. The dummy package does make it easier, however.\n\n# Convert every factor type feature into \n# a collection dummy variables.\nbikes_dummies = dummy(bikes, int = TRUE)\n\nBefore running the dummy() function we had 10 variables in the dataset. The result of the function is a new dataset with only the dummy variables generated from the factor variables in bikes. At this point we can replace the factor variables with the dummy ones.\n\nbikes_num = bikes %&gt;% keep(is.numeric)\nbikes = bind_cols(bikes_num, bikes_dummies)"
  },
  {
    "objectID": "posts/Problem Set 2/index.html#step-6",
    "href": "posts/Problem Set 2/index.html#step-6",
    "title": "Problem Set 2 | ",
    "section": "Step 6",
    "text": "Step 6\nWe’re going to perform a penalized form of regression known as LASSO to find a decent predictive model. We’ll need to do a few things first. We need to get rid variables we don’t intend to have as predictors. The date and realfeel features will be removed.\n\nbikes = bikes %&gt;%\n  select(-realfeel) %&gt;%\n  mutate(temperature2 = temperature^2)\n\nNormally, for a linear regression, you’d need to remove one dummy variable from a categorical variable. For example, season has 4 values (Winter, Spring, Fall, and Summer). We have dummy variable for each, but we need to omit one in order for it to work. But with LASSO, its okay and actually better to include them all and let the algorithm decide which to eliminate.\n\n# Separate predictors from target feature.\nrentals = bikes$rentals\npredictors = as.matrix(select(bikes, -rentals))\n  \n\n# estimate model\ncv.model = gamlr::cv.gamlr(x=predictors, y=rentals)\n\nLoading required package: gamlr\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nplot(cv.model)\n\n\n\n\n\nbetamin = coef(cv.model, select = \"min\")\nbetamin\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     seg91\nintercept      7188.241247\ntemperature     977.913810\nhumidity      -2989.755533\nwindspeed     -1863.339998\nseason_Winter  -727.909032\nseason_Spring   -95.984494\nseason_Summer    12.909978\nseason_Fall     301.456207\nholiday_0       437.471632\nholiday_1         .       \nweekday_0      -260.794743\nweekday_1       -96.389630\nweekday_2        -5.296943\nweekday_3         .       \nweekday_4         .       \nweekday_5         .       \nweekday_6        58.890335\nweather_1       256.135832\nweather_2         .       \nweather_3     -1658.716768\ntemperature2   -529.867870\n\n\n\nbikes = bikes %&gt;%\n  mutate(pred = as.numeric(predict(cv.model, predictors)))\n\n\nbikes %&gt;%\n  ggplot(aes(x=rentals, y=pred)) +\n  geom_point()"
  },
  {
    "objectID": "posts/Problem Set 3/index.html",
    "href": "posts/Problem Set 3/index.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#r-markdown",
    "href": "posts/Problem Set 3/index.html#r-markdown",
    "title": "Problem Set 3",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#including-plots",
    "href": "posts/Problem Set 3/index.html#including-plots",
    "title": "Problem Set 3",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\n\nAssignment\n1. Create a new Rmarkdown document that will show case your data exploration, modeling, and error testing.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rpart)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n2. Explore the data and determine the number of variables and the quantity of any missing values. If values are missing, prescribe a plan to deal with the problem.\n\nToycor &lt;- read.csv(\"ToyotaCorolla.csv\")\nstr(Toycor)\n\n'data.frame':   1436 obs. of  39 variables:\n $ Id               : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Model            : chr  \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" ...\n $ Price            : int  13500 13750 13950 14950 13750 12950 16900 18600 21500 12950 ...\n $ Age_08_04        : int  23 23 24 26 30 32 27 30 27 23 ...\n $ Mfg_Month        : int  10 10 9 7 3 1 6 3 6 10 ...\n $ Mfg_Year         : int  2002 2002 2002 2002 2002 2002 2002 2002 2002 2002 ...\n $ KM               : int  46986 72937 41711 48000 38500 61000 94612 75889 19700 71138 ...\n $ Fuel_Type        : chr  \"Diesel\" \"Diesel\" \"Diesel\" \"Diesel\" ...\n $ HP               : int  90 90 90 90 90 90 90 90 192 69 ...\n $ Met_Color        : int  1 1 1 0 0 0 1 1 0 0 ...\n $ Color            : chr  \"Blue\" \"Silver\" \"Blue\" \"Black\" ...\n $ Automatic        : int  0 0 0 0 0 0 0 0 0 0 ...\n $ CC               : int  2000 2000 2000 2000 2000 2000 2000 2000 1800 1900 ...\n $ Doors            : int  3 3 3 3 3 3 3 3 3 3 ...\n $ Cylinders        : int  4 4 4 4 4 4 4 4 4 4 ...\n $ Gears            : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Quarterly_Tax    : int  210 210 210 210 210 210 210 210 100 185 ...\n $ Weight           : int  1165 1165 1165 1165 1170 1170 1245 1245 1185 1105 ...\n $ Mfr_Guarantee    : int  0 0 1 1 1 0 0 1 0 0 ...\n $ BOVAG_Guarantee  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Guarantee_Period : int  3 3 3 3 3 3 3 3 3 3 ...\n $ ABS              : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_1         : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_2         : int  1 1 1 1 1 1 1 1 0 1 ...\n $ Airco            : int  0 1 0 0 1 1 1 1 1 1 ...\n $ Automatic_airco  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Boardcomputer    : int  1 1 1 1 1 1 1 1 0 1 ...\n $ CD_Player        : int  0 1 0 0 0 0 0 1 0 0 ...\n $ Central_Lock     : int  1 1 0 0 1 1 1 1 1 0 ...\n $ Powered_Windows  : int  1 0 0 0 1 1 1 1 1 0 ...\n $ Power_Steering   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Radio            : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Mistlamps        : int  0 0 0 0 1 1 0 0 0 0 ...\n $ Sport_Model      : int  0 0 0 0 0 0 1 0 0 0 ...\n $ Backseat_Divider : int  1 1 1 1 1 1 1 1 0 1 ...\n $ Metallic_Rim     : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Radio_cassette   : int  0 0 0 0 0 0 0 0 1 0 ...\n $ Parking_Assistant: int  0 0 0 0 0 0 0 0 0 0 ...\n $ Tow_Bar          : int  0 0 0 0 0 0 0 0 0 0 ...\n\n\n\nmissing_values &lt;- colSums(is.na(Toycor))\n\n3. Analyze whether the Price variable is appropriate for a linear regression model and discuss its distribution. Are there any transformations that we might apply to the price variable?\n\nplot(Toycor$Price, Toycor$IndependentVariable, main = \"Scatter Plot of Price vs. IndependentVariable\")\n\n\n\n\n\nToycor$LogPrice &lt;- log(Toycor$Price)\n\n4. Is there a relationship between any of the features in the data and the Price feature? Perform some exploratory analysis to determine some features that are related using a feature plot.\n\nToycor = Toycor %&gt;%\n  select(-Id, -Model, -Mfg_Month, -Cylinders)\n\n\nToycor_fct = Toycor %&gt;%\n  select(-Price, -Age_08_04, -KM, -HP, -CC, -Quarterly_Tax, -Weight) %&gt;%\n  mutate_all(.funs = factor)\n\nToycor_num = Toycor %&gt;%\n  select(Price, Age_08_04, KM, HP, CC, Quarterly_Tax, Weight)\n\nToycor2 = bind_cols(Toycor_num, Toycor_fct)\n\n\nToycor2 %&gt;%\n  keep(is.numeric) %&gt;%\n  ggpairs()\n\n\n\n\n\ncorrelation_mileage &lt;- cor(Toycor$KM, Toycor$Price)\ncorrelation_age &lt;- cor(Toycor$Age_08_04, Toycor$Price)\ncorrelation_Mfg_Year &lt;- cor(Toycor$Mfg_Year, Toycor$Price)\n\n\nlibrary(ggplot2)\n\nggplot(Toycor, aes(x = KM, y = Price)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Price vs. KM\")\n\n\n\nggplot(Toycor, aes(x = Age_08_04, y = Price)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Price vs. Age_08_04\")\n\n\n\nggplot(Toycor, aes(x = Mfg_Year, y = Price)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Price vs. Mfg_Year\")\n\n\n\n\n\nlm1 = lm(Price ~ Age_08_04 + KM,\n         data = Toycor2)\nsummary(lm1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM, data = Toycor2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6789.7  -971.9   -63.7   828.0 12633.5 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.048e+04  1.400e+02  146.26   &lt;2e-16 ***\nAge_08_04   -1.541e+02  2.736e+00  -56.33   &lt;2e-16 ***\nKM          -1.646e-02  1.357e-03  -12.13   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1663 on 1433 degrees of freedom\nMultiple R-squared:   0.79, Adjusted R-squared:  0.7897 \nF-statistic:  2695 on 2 and 1433 DF,  p-value: &lt; 2.2e-16\n\n\n\nToycor %&gt;%\n  select(-Fuel_Type, -Color) %&gt;%\n  cor() %&gt;%\n  corrplot::corrplot(., number.cex=.2)\n\n\n\n\n5. Are there any predictor variables in the data that are potentially too strongly related to each other? Make sure to use reference any visualizations, tables, or numbers to show this.\nAge_08_04 and Price are strongly negatively correlated. You can see this relationship in the scatter plot below. The negative slope of the trend line shows the negative correlation. The correlation of -0.877 between Age_08_04 and Price is the only relationship that is potentially too strong.\n\nggplot(Toycor, aes(x = Age_08_04, y = Price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +  # Add a trendline (linear regression)\n  labs(title = \"Scatter Plot of Price vs. Age_08_04\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n6. Partition your data into a training set with 70% of the observations and a testing set with the remaining 30%.\n\nset.seed(123)  \ntrainIndex &lt;- createDataPartition(Toycor$Price, p = 0.7, list = FALSE)\ntrain_data &lt;- Toycor[trainIndex, ]\ntest_data &lt;- Toycor[-trainIndex, ]\n\n7. Based on your results and relationships in questions (4) and (5), build a regression tree model to predict car prices. With the data Toycor_data Make sure to conduct cross validation to evaluate the model and choose the best cost complexity parameter for this problem (use default values for minsplit, minbucket, maxdepth, etc. But choose grid of cp values to tune over). Use rpart.plot to view your tree and discuss its complexity, usefulness, etc. What role is pre-pruning and post-pruning playing here?\n\ncp_seq &lt;- seq(0.01, 0.1, by = 0.01)\n\n8. Look at the feature importance (using permuted feature importance in “iml” package, with loss = “rmse” and compare = “ratio”) and determine which features have the biggest effect, and which might be okay to remove.\n9. Parsimony is about obtaining the simplest model possible, without oversimplifying. Remove a few of the less useful features and retrain / cross validate / tune your tree.\n10. Use the model resulting from question 9 and test predictions on the testing data. Compare the cross validation error and and testing data. Spend some time interpreting what this prediction error means for your pricing model and its use for CorollaCrowd."
  }
]