---
title: "Problem Set 3"
author: "Isaac Baron"
date: "2023-10-08"
format: html
---

#### **Assignment**

1\. Create a new Rmarkdown document that will show case your data exploration, modeling, and error testing.

```{r}
library(tidyverse)
library(rpart)
library(caret)
library(GGally)
```

```{r}
library(rpart.plot)
```

2\. Explore the data and determine the number of variables and the quantity of any missing values. If values are missing, prescribe a plan to deal with the problem.

```{r}
Toycor <- read.csv("ToyotaCorolla.csv")
str(Toycor)


```

```{r}
missing_values <- colSums(is.na(Toycor))
```

3\. Analyze whether the Price variable is appropriate for a linear regression model and discuss its distribution. Are there any transformations that we might apply to the price variable?

```{r}

plot(Toycor$Price, Toycor$IndependentVariable, main = "Scatter Plot of Price vs. IndependentVariable")

```

```{r}
Toycor$LogPrice <- log(Toycor$Price)

```

4\. Is there a relationship between any of the features in the data and the Price feature? Perform some exploratory analysis to determine some features that are related using a feature plot.

```{r}
Toycor = Toycor %>%
  select(-Id, -Model, -Mfg_Month, -Cylinders)
```

```{r}
Toycor_fct = Toycor %>%
  select(-Price, -Age_08_04, -KM, -HP, -CC, -Quarterly_Tax, -Weight) %>%
  mutate_all(.funs = factor)

Toycor_num = Toycor %>%
  select(Price, Age_08_04, KM, HP, CC, Quarterly_Tax, Weight)

Toycor2 = bind_cols(Toycor_num, Toycor_fct)

```

```{r}
Toycor2 %>%
  keep(is.numeric) %>%
  ggpairs()
```

```{r}
correlation_mileage <- cor(Toycor$KM, Toycor$Price)
correlation_age <- cor(Toycor$Age_08_04, Toycor$Price)
correlation_Mfg_Year <- cor(Toycor$Mfg_Year, Toycor$Price)
```

```{r}
library(ggplot2)

ggplot(Toycor, aes(x = KM, y = Price)) +
  geom_point() +
  labs(title = "Scatter Plot of Price vs. KM")

ggplot(Toycor, aes(x = Age_08_04, y = Price)) +
  geom_point() +
  labs(title = "Scatter Plot of Price vs. Age_08_04")

ggplot(Toycor, aes(x = Mfg_Year, y = Price)) +
  geom_point() +
  labs(title = "Scatter Plot of Price vs. Mfg_Year")
```

```{r}
lm1 = lm(Price ~ Age_08_04 + KM,
         data = Toycor2)
summary(lm1)

```

```{r}
Toycor %>%
  select(-Fuel_Type, -Color) %>%
  cor() %>%
  corrplot::corrplot(., number.cex=.2)
```

5\. Are there any predictor variables in the data that are potentially too strongly related to each other? Make sure to use reference any visualizations, tables, or numbers to show this.

**Age_08_04** and **Price** are strongly negatively correlated. You can see this relationship in the scatter plot below. The negative slope of the trend line shows the negative correlation. The correlation of -0.877 between **Age_08_04** and **Price** is the only relationship that is potentially too strong.

```{r}
ggplot(Toycor, aes(x = Age_08_04, y = Price)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Add a trendline (linear regression)
  labs(title = "Scatter Plot of Price vs. Age_08_04")

```

6\. Partition your data into a training set with 70% of the observations and a testing set with the remaining 30%.

```{r}
set.seed(123)  
trainIndex <- createDataPartition(Toycor$Price, p = 0.7, list = FALSE)
train_data <- Toycor[trainIndex, ]
test_data <- Toycor[-trainIndex, ]

```

7\. Based on your results and relationships in questions (4) and (5), build a regression tree model to predict car prices. With the data Toycor_data Make sure to conduct cross validation to evaluate the model and choose the best cost complexity parameter for this problem (use default values for minsplit, minbucket, maxdepth, etc. But choose grid of cp values to tune over). Use rpart.plot to view your tree and discuss its complexity, usefulness, etc. What role is pre-pruning and post-pruning playing here?

```{r}
dt1 = train(Price ~ .,
            data = select(test_data, -Age_08_04),
            method = "rpart",
            trControl = trainControl(method = "none"),
            metric = "RMSE")
dt1
```

```{r}

# Define a formula for the model
formula <- Price ~ .

# Set up a grid of 'cp' values to tune over
cp_grid <- seq(0, 0.1, by = 0.01)

# Create a train control object for cross-validation (e.g., 10-fold)
ctrl <- trainControl(method = "cv", number = 10)

# Train the regression tree model and choose the best 'cp' value
dt_model <- train(
  formula,
  data = train_data,
  method = "rpart",
  trControl = ctrl,
  tuneGrid = data.frame(cp = cp_grid),
  metric = "RMSE"
)

# View the best 'cp' value and summary
cat("Best cp value:", dt_model$bestTune$cp, "\n")
printcp(dt_model$finalModel)

# Plot the best tree using rpart.plot
best_tree <- rpart(formula, data = train_data, control = rpart.control(cp = dt_model$bestTune$cp))
rpart.plot(best_tree, box.palette = "auto")
```

8\. Look at the feature importance (using permuted feature importance in "iml" package, with loss = "rmse" and compare = "ratio") and determine which features have the biggest effect, and which might be okay to remove.

```{r}
# Load the necessary libraries
library(caret)

# Feature importance using caret's rpart method
feature_importance <- varImp(dt1, scale = FALSE)

# View feature importance
print(feature_importance)




```

9\. Parsimony is about obtaining the simplest model possible, without oversimplifying. Remove a few of the less useful features and retrain / cross validate / tune your tree.

```{r}
train_data <- train_data %>% select(-Tow_Bar,-Automatic,-Metallic_Rim,-Weight,-CC,-Power_Steering)
test_data <- test_data %>% select(-Tow_Bar,-Automatic,-Metallic_Rim,-Weight,-CC,-Power_Steering)

# Retrain the regression tree model with cross-validation
dt2 = train(Price ~ .,
             data = train_data,
             method = "rpart",
             trControl = trainControl(method = "cv", number = 10),
             metric = "RMSE")


```

10\. Use the model resulting from question 9 and test predictions on the testing data. Compare the cross validation error and and testing data. Spend some time interpreting what this prediction error means for your pricing model and its use for CorollaCrowd.

```{r}
# Predict on the testing data using the model from step 9
test_predictions2 <- predict(dt2, newdata = test_data)

# Calculate RMSE on the testing data
test_rmse2 <- sqrt(mean((test_predictions2 - test_data$Price)^2))

# Cross-validation RMSE from the model in step 9
cv_rmse2 <- dt2$results$RMSE

# Compare cross-validation error and testing error
cat("Cross-Validation RMSE:", cv_rmse2, "\n")
cat("Testing RMSE:", test_rmse2, "\n")

# Interpretation of prediction error:
# Compare the cross-validation RMSE with the testing RMSE. A lower RMSE indicates better predictive performance.
# Evaluate whether the testing RMSE is acceptable for your application and consider potential improvements if necessary.




```
